# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021 Ant Group Co., Ltd.
# This file is distributed under the same license as the SPU package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SPU \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-03-13 15:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../development/policy_sgd_insight.rst:2
msgid "Background: SGD in MPC-ML"
msgstr ""

#: ../../development/policy_sgd_insight.rst:4
msgid ""
"SGD(Stochastic Gradient Descent) is a famous optimization algorithm, it "
"updates weights using gradient direction. However, it suffers that user "
"should choose hyper-parameters very carefully. Of course, grid-search is "
"a potential treatment for this problem, but it becomes impractical when "
"training cost is large. As an example, When running LR with SGD in "
"`credit card dataset <https://www.kaggle.com/datasets/uciml/default-of-"
"credit-card-clients-dataset>`_ (for 4096-0.1, 4096 is batch_size, 0.1 is "
"learning_rate), we can find that it seems safer to use small batch_size "
"and learning_rate, else we get some loss or very strong vibration of auc "
"within 10 epochs(we leave 10000 samples as test dataset randomly)."
msgstr ""

#: ../../development/policy_sgd_insight.rst:12
msgid ""
"Unfortunately, when under MPC, even simple algorithm like SSLR, small "
"batch_size leads to huge training time under limited network resources. "
"Besides, even you have high bandwidth, small batch_size can not utilize "
"it!"
msgstr ""

#: ../../development/policy_sgd_insight.rst:16
msgid "How to improve SGD"
msgstr ""

#: ../../development/policy_sgd_insight.rst:17
msgid ""
"Indeed, after our elaborated experiments, we can find two drawbacks of "
"naive SGD:"
msgstr ""

#: ../../development/policy_sgd_insight.rst:19
msgid "slow update(because of small learning_rate) at the beginning."
msgstr ""

#: ../../development/policy_sgd_insight.rst:21
msgid "vibration happens when near to optimal point."
msgstr ""

#: ../../development/policy_sgd_insight.rst:23
msgid ""
"So, it's a straight strategy to use \"large\" learning_rate at the "
"beginning, and \"slow down\" as training goes on. But the ensuing "
"question is how to determine specific \"large\" and \"slow down\" for "
"different datasets."
msgstr ""

#: ../../development/policy_sgd_insight.rst:27
msgid "What's Policy sgd"
msgstr ""

#: ../../development/policy_sgd_insight.rst:28
msgid ""
"For SSLR, we provide an answer to the above question: policy-sgd(already "
"implemented on `SSRegression` in Secretflow when setting "
"`strategy=policy_sgd`)."
msgstr ""

#: ../../development/policy_sgd_insight.rst:33
msgid "The core of this optimizer consists of two parts:"
msgstr ""

#: ../../development/policy_sgd_insight.rst:35
#, python-brace-format
msgid ""
"1. Adaptive learning rate scaling mechanism: in first epoch, we force the"
" weight move unit-norm in gradient direction and record "
":math:`\\frac{1}{||grad||_2}` as scale factor for this batch."
msgstr ""

#: ../../development/policy_sgd_insight.rst:38
msgid ""
"2. Learning rate decay and early stop: we use step-decay strategy for "
"learning_rate decay. As for early stop, we compare two strategies, loss "
"based(use Taylor expansion to avoid invoking time-consuming op like "
"`exp`, `log`) and weight based, and choose the latter in our final "
"implementation."
msgstr ""

#: ../../development/policy_sgd_insight.rst:44
msgid "Experiments"
msgstr ""

#: ../../development/policy_sgd_insight.rst:45
#, python-brace-format
msgid ""
"We use 4 dataset for experiments, containing 3 open source dataset "
"(`credit_card <https://www.kaggle.com/datasets/uciml/default-of-credit-"
"card-clients-dataset>`_, `Bank Marketing "
"<https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#>`_, `Aps "
"<https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks>`_)"
" and 1 business dataset(wzdata). For open source dataset, we just do some"
" basic one-hot and min-max normalization like normal LR needs. We leave "
"out about :math:`\\frac{1}{3}` data for test data and evaluate auc on it."
" The baseline auc is computed when using sklearn for model training."
msgstr ""

#: ../../development/policy_sgd_insight.rst:53
#: ../../development/policy_sgd_insight.rst:107
msgid "Dataset"
msgstr ""

#: ../../development/policy_sgd_insight.rst:53
msgid "Training shape"
msgstr ""

#: ../../development/policy_sgd_insight.rst:53
msgid "baseline auc"
msgstr ""

#: ../../development/policy_sgd_insight.rst:55
#: ../../development/policy_sgd_insight.rst:109
msgid "business dataset"
msgstr ""

#: ../../development/policy_sgd_insight.rst:55
msgid "111618，23"
msgstr ""

#: ../../development/policy_sgd_insight.rst:55
msgid "0.8175"
msgstr ""

#: ../../development/policy_sgd_insight.rst:57
#: ../../development/policy_sgd_insight.rst:111
msgid "Bank Marketing"
msgstr ""

#: ../../development/policy_sgd_insight.rst:57
msgid "40787，48"
msgstr ""

#: ../../development/policy_sgd_insight.rst:57
msgid "0.93"
msgstr ""

#: ../../development/policy_sgd_insight.rst:59
#: ../../development/policy_sgd_insight.rst:113
msgid "credit_card"
msgstr ""

#: ../../development/policy_sgd_insight.rst:59
msgid "20000, 23"
msgstr ""

#: ../../development/policy_sgd_insight.rst:59
msgid "0.718"
msgstr ""

#: ../../development/policy_sgd_insight.rst:61
#: ../../development/policy_sgd_insight.rst:115
msgid "Aps"
msgstr ""

#: ../../development/policy_sgd_insight.rst:61
msgid "60000，170"
msgstr ""

#: ../../development/policy_sgd_insight.rst:61
msgid "0.9666"
msgstr ""

#: ../../development/policy_sgd_insight.rst:65
msgid "Precision"
msgstr ""

#: ../../development/policy_sgd_insight.rst:67
msgid ""
"We first test how precision of fixed point influence SSLR and test three "
"settings:"
msgstr ""

#: ../../development/policy_sgd_insight.rst:69
msgid "low precision: `FM64` + 18 fxp"
msgstr ""

#: ../../development/policy_sgd_insight.rst:71
msgid "medium precision: `FM128` + 28 fxp"
msgstr ""

#: ../../development/policy_sgd_insight.rst:73
msgid "high precision: `FM128` + 42 fxp"
msgstr ""

#: ../../development/policy_sgd_insight.rst:81
msgid ""
"We can find that for both optimizer, precision has little influence on "
"final auc, so it's safe for user to choose low precision when training "
"LR."
msgstr ""

#: ../../development/policy_sgd_insight.rst:85
msgid "Naive v.s Policy"
msgstr ""

#: ../../development/policy_sgd_insight.rst:87
msgid ""
"Then, we compare the totally runtime of naive_sgd(v1) and policy_sgd(v2)."
" For naive-sgd, we follow the \"safe strategy\"(mostly used in plaintext "
"ML): small learning_rate like 0.1, and small batch_size like 1024(If "
"using 2048, then some data does not converge). Also, it's hard to decide "
"a good default value for naive_sgd to early stop well(even worse, you may"
" get huge auc drop if setting bad values). To avoid tedious grid-search, "
"so for naive_sgd, it runs without any learning_rate decay(recommended way"
" for naive_sgd). But for policy_sgd, it's often harmless to use larger "
"batch_size(2048 for these experiments),and we set learning_rate decay a "
"half every 2 epochs."
msgstr ""

#: ../../development/policy_sgd_insight.rst:93
msgid ""
"As for other hyper-parameters, we set total running epochs to 20, "
"learning_rate to 0.1 and use low precision, CHEETAH protocol. And we test"
" in WAN, providing 20Mbps and 20ms RTT, which is a typical setting in "
"real world project."
msgstr ""

#: ../../development/policy_sgd_insight.rst:99
msgid ""
"First, we find for naive_sgd(v1), none of them meets any early stop "
"criterion during 20 epochs(so we omit the early stop line in figure). "
"However, for policy_sgd(v2), it can always \"stop well\"(red dot line "
"means policy_sgd meets the stop criterion based on loss, similar for "
"purple line) after the model converges. Besides, checking the auc of "
"stopping time, it has very low gap(<0.01) between baseline."
msgstr ""

#: ../../development/policy_sgd_insight.rst:103
msgid ""
"The following table shows the total running time of policy_sgd and "
"naive_sgd(based on weight early stop). Policy_sgd can reduce the time by "
"2-5 times compared to naive_sgd."
msgstr ""

#: ../../development/policy_sgd_insight.rst:107
msgid "naive_sgd(s)"
msgstr ""

#: ../../development/policy_sgd_insight.rst:107
msgid "policy_sgd(s)"
msgstr ""

#: ../../development/policy_sgd_insight.rst:107
msgid "naive/policy"
msgstr ""

#: ../../development/policy_sgd_insight.rst:109
msgid "~8000"
msgstr ""

#: ../../development/policy_sgd_insight.rst:109
#: ../../development/policy_sgd_insight.rst:113
msgid "~1600"
msgstr ""

#: ../../development/policy_sgd_insight.rst:109
msgid "5x"
msgstr ""

#: ../../development/policy_sgd_insight.rst:111
msgid "~3000"
msgstr ""

#: ../../development/policy_sgd_insight.rst:111
msgid "~800"
msgstr ""

#: ../../development/policy_sgd_insight.rst:111
msgid "3.75x"
msgstr ""

#: ../../development/policy_sgd_insight.rst:113
msgid "~350"
msgstr ""

#: ../../development/policy_sgd_insight.rst:113
msgid "4.57x"
msgstr ""

#: ../../development/policy_sgd_insight.rst:115
msgid "~10000"
msgstr ""

#: ../../development/policy_sgd_insight.rst:115
msgid "~4200"
msgstr ""

#: ../../development/policy_sgd_insight.rst:115
msgid "2.38x"
msgstr ""

