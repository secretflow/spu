{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Develop Your First MPC-Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> The following codes are demos only. It’s NOT for production due to system security concerns, please DO NOT use it directly in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is an introductory secretflow tutorial that contains:\n",
    "\n",
    "* Implement a simple algorithm and run it in plaintext as baseline.\n",
    "* Use simulator to check the **precision loss** and try to fix it.\n",
    "* Run elaborated emulations to give reports on both **efficiency and correctness**.\n",
    "\n",
    "We **highly recommend** the reader to read [spu-quickstart](https://www.secretflow.org.cn/docs/spu/en/getting_started/quick_start.html#SPU-Quickstart) before continuing read this tutorial, which you can learn some basic usage of Device, DeviceObject and how to run program in secret."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 0: Prepare the environment and dataset\n",
    "1. Environment: To run this tutorial, you should have spu installed in your environment(if not, you can refer to [this](https://www.secretflow.org.cn/docs/spu/en/getting_started/install.html)).\n",
    "2. Dataset: We use the breast cancer wisconsin dataset, which can be obtained from sklearn. And we just do simple minmax transform for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "# normally, LR works only when the features have been normalized!\n",
    "scalar = MinMaxScaler(feature_range=(-2, 2))\n",
    "cols = X.columns\n",
    "X = scalar.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.084150</td>\n",
       "      <td>-1.909368</td>\n",
       "      <td>0.183954</td>\n",
       "      <td>-0.545069</td>\n",
       "      <td>0.375011</td>\n",
       "      <td>1.168149</td>\n",
       "      <td>0.812559</td>\n",
       "      <td>0.924453</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.422072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483102</td>\n",
       "      <td>-1.433902</td>\n",
       "      <td>0.673241</td>\n",
       "      <td>-0.197208</td>\n",
       "      <td>0.404543</td>\n",
       "      <td>0.477166</td>\n",
       "      <td>0.274441</td>\n",
       "      <td>1.648110</td>\n",
       "      <td>0.393850</td>\n",
       "      <td>-0.324544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.572578</td>\n",
       "      <td>-0.909706</td>\n",
       "      <td>0.463133</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>-0.840480</td>\n",
       "      <td>-1.272928</td>\n",
       "      <td>-1.185567</td>\n",
       "      <td>-0.604970</td>\n",
       "      <td>-0.480808</td>\n",
       "      <td>-1.434709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427606</td>\n",
       "      <td>-0.785714</td>\n",
       "      <td>0.159271</td>\n",
       "      <td>-0.259143</td>\n",
       "      <td>-0.609787</td>\n",
       "      <td>-1.381747</td>\n",
       "      <td>-1.228115</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>-1.065642</td>\n",
       "      <td>-1.108487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.405982</td>\n",
       "      <td>-0.438958</td>\n",
       "      <td>0.382973</td>\n",
       "      <td>-0.202333</td>\n",
       "      <td>0.057236</td>\n",
       "      <td>-0.275934</td>\n",
       "      <td>-0.149953</td>\n",
       "      <td>0.542744</td>\n",
       "      <td>0.038384</td>\n",
       "      <td>-1.155013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225543</td>\n",
       "      <td>-0.559701</td>\n",
       "      <td>0.033767</td>\n",
       "      <td>-0.501966</td>\n",
       "      <td>-0.065641</td>\n",
       "      <td>-0.458499</td>\n",
       "      <td>-0.561022</td>\n",
       "      <td>1.340206</td>\n",
       "      <td>-0.385176</td>\n",
       "      <td>-1.146268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.159638</td>\n",
       "      <td>-0.556645</td>\n",
       "      <td>-1.065994</td>\n",
       "      <td>-1.588378</td>\n",
       "      <td>1.245283</td>\n",
       "      <td>1.245445</td>\n",
       "      <td>0.262418</td>\n",
       "      <td>0.091451</td>\n",
       "      <td>1.105051</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.006759</td>\n",
       "      <td>-0.456290</td>\n",
       "      <td>-1.034613</td>\n",
       "      <td>-1.623968</td>\n",
       "      <td>1.661890</td>\n",
       "      <td>1.256047</td>\n",
       "      <td>0.194569</td>\n",
       "      <td>1.539519</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.094845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.519570</td>\n",
       "      <td>-1.373690</td>\n",
       "      <td>0.523944</td>\n",
       "      <td>-0.042842</td>\n",
       "      <td>-0.278595</td>\n",
       "      <td>-0.608429</td>\n",
       "      <td>-0.144330</td>\n",
       "      <td>0.073559</td>\n",
       "      <td>-0.486869</td>\n",
       "      <td>-1.252738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078975</td>\n",
       "      <td>-1.504264</td>\n",
       "      <td>0.027790</td>\n",
       "      <td>-0.633700</td>\n",
       "      <td>-0.250545</td>\n",
       "      <td>-1.310339</td>\n",
       "      <td>-0.722045</td>\n",
       "      <td>0.233677</td>\n",
       "      <td>-1.369998</td>\n",
       "      <td>-1.429621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0     0.084150     -1.909368        0.183954  -0.545069         0.375011   \n",
       "1     0.572578     -0.909706        0.463133   0.006363        -0.840480   \n",
       "2     0.405982     -0.438958        0.382973  -0.202333         0.057236   \n",
       "3    -1.159638     -0.556645       -1.065994  -1.588378         1.245283   \n",
       "4     0.519570     -1.373690        0.523944  -0.042842        -0.278595   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0          1.168149        0.812559             0.924453       0.745455   \n",
       "1         -1.272928       -1.185567            -0.604970      -0.480808   \n",
       "2         -0.275934       -0.149953             0.542744       0.038384   \n",
       "3          1.245445        0.262418             0.091451       1.105051   \n",
       "4         -0.608429       -0.144330             0.073559      -0.486869   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                0.422072  ...      0.483102      -1.433902         0.673241   \n",
       "1               -1.434709  ...      0.427606      -0.785714         0.159271   \n",
       "2               -1.155013  ...      0.225543      -0.559701         0.033767   \n",
       "3                2.000000  ...     -1.006759      -0.456290        -1.034613   \n",
       "4               -1.252738  ...      0.078975      -1.504264         0.027790   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0   -0.197208          0.404543           0.477166         0.274441   \n",
       "1   -0.259143         -0.609787          -1.381747        -1.228115   \n",
       "2   -0.501966         -0.065641          -0.458499        -0.561022   \n",
       "3   -1.623968          1.661890           1.256047         0.194569   \n",
       "4   -0.633700         -0.250545          -1.310339        -0.722045   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0              1.648110        0.393850                -0.324544  \n",
       "1              0.556701       -1.065642                -1.108487  \n",
       "2              1.340206       -0.385176                -1.146268  \n",
       "3              1.539519        2.000000                 1.094845  \n",
       "4              0.233677       -1.369998                -1.429621  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 1: Implement algorithm in plaintext\n",
    "\n",
    "[SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)(Stochastic Gradient Descent) is a simple but effective optimization algorithm, so in MPC settings, it's common to use it to optimize the model.\n",
    "\n",
    "[LR](https://en.wikipedia.org/wiki/Logistic_regression)(Logistic Regression) is a widely used linear model especially in financial industry. So in this tutorial, as an example, we will implement LR with a modified SGD, called [policy-sgd](./policy_sgd_insight.rst), which can accelerate the speeds of training in most scenery.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, we just list some important equations used in policy-sgd:\n",
    "- LR compute gradient with(`n` is batch_size):\n",
    "$$ grad = \\frac{1}{n} \\sum_{i} (sigmoid(w^T x_i) - y_i) x_i $$\n",
    "- Policy-sgd compute dk in first epoch with(`p` is number of features):\n",
    "$$ d_k = \\frac{1}{\\sqrt{\\sum_j^{p} grad_j^2} + \\epsilon} $$\n",
    "- Then, update weights with(`i` means i-th epoch, `k` means k-th iter):\n",
    "$$ w_{i,k} = w_{i, k-1} -  d_k * \\alpha *  grad $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this part, we first forget the MPC setting(data split, protocol...) and  implement the algorithm in plaintext. Secretflow recommends user to do this with [Jax](https://jax.readthedocs.io/en/latest/), which `jax.numpy` provides a familiar NumPy-style API for ease of adoption. If you are familiar with Numpy, you can go through [this blog](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) and gets some caveats and then write jax-code just like numpy-code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import some basic library\n",
    "# use jnp just like np\n",
    "import jax.numpy as jnp\n",
    "import jax.lax\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The original response function for LR is sigmoid function, which contains time-consuming ops like exp and division in MPC. So it's common to approximate sigmoid function with other MPC-friendly function. Here we give two method, i.e. first-order Taylor and square root approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_t1(x, limit: bool = True):\n",
    "    '''\n",
    "    taylor series referenced from:\n",
    "    https://mortendahl.github.io/2017/04/17/private-deep-learning-with-mpc/\n",
    "    '''\n",
    "    T0 = 1.0 / 2\n",
    "    T1 = 1.0 / 4\n",
    "    ret = T0 + x * T1\n",
    "    if limit:\n",
    "        return jnp.select([ret < 0, ret > 1], [0, 1], ret)\n",
    "    else:\n",
    "        return ret\n",
    "\n",
    "\n",
    "def sigmoid_sr(x):\n",
    "    \"\"\"\n",
    "    https://en.wikipedia.org/wiki/Sigmoid_function#Examples\n",
    "    Square Root approximation functions:\n",
    "    F(x) = 0.5 * ( x / ( 1 + x^2 )^0.5 ) + 0.5\n",
    "    sigmoid_sr almost perfect fit to sigmoid if x out of range [-3,3]\n",
    "    highly recommended use this appr as GDBT's default sigmoid method.\n",
    "    \"\"\"\n",
    "    return 0.5 * (x / jnp.sqrt(1 + jnp.power(x, 2))) + 0.5\n",
    "\n",
    "\n",
    "def sigmoid(x, method='t1'):\n",
    "    if method == 't1':\n",
    "        return sigmoid_t1(x)\n",
    "    else:\n",
    "        return sigmoid_sr(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "policy-sgd needs scale learning rate in first epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: we leave a method param in this function for next part, in plaintext, we won't invoke low-level op in most conditions.\n",
    "def compute_dk_func(x, eps=1e-6, method='norm'):\n",
    "    # Same as Adam, need add small eps to avoid zero-division error\n",
    "    if method == 'norm':\n",
    "        return 1 / (jnp.linalg.norm(x) + eps)\n",
    "    else:\n",
    "        # invoke low-level rsqrt op by hand\n",
    "        return jax.lax.rsqrt(jnp.sum(jnp.square(x)) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then, we give a brief implementation of LR with policy-sgd, and have similar interface(but less) with sklearn.\n",
    "\n",
    "**Note**: for simplicity, we will always fit intercept in LR model and omit regularization and other techniques. For full version of SSLR,  can refer to `SSRegression` in secretflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SSLRSGDClassifier:\n",
    "    def __init__(\n",
    "            self,\n",
    "            epochs: int,\n",
    "            learning_rate: float,\n",
    "            batch_size: int,\n",
    "            sig_type: str = 't1',\n",
    "            eps: float = 1e-6,  # eps is the small number for computing dk\n",
    "            dk_method: str = 'norm',  # method to compute dk, default is use jnp.linalg.norm function\n",
    "    ):\n",
    "        # parameter check.\n",
    "        assert epochs > 0, f\"epochs should >0\"\n",
    "        assert learning_rate > 0, f\"learning_rate should >0\"\n",
    "        assert batch_size > 0, f\"batch_size should >0\"\n",
    "        assert sig_type in ['t1', 'sr'], f\"sig_type should one of ['t1', 'sr']\"\n",
    "        assert eps > 0, f\"eps should >0\"\n",
    "        assert dk_method in ['norm', 'rsqrt'], f\"dk_method should one of ['norm', 'rsqrt']\"\n",
    "\n",
    "        self._epochs = epochs\n",
    "        self._learning_rate = learning_rate\n",
    "        self._batch_size = batch_size\n",
    "        self._sig_type = sig_type\n",
    "        self._eps = eps\n",
    "        self._dk_method = dk_method\n",
    "\n",
    "        self._weights = jnp.zeros(())\n",
    "\n",
    "    def _update_weights(\n",
    "            self,\n",
    "            x,  # array-like\n",
    "            y,  # array-like\n",
    "            w,  # array-like\n",
    "            total_batch: int,\n",
    "            batch_size: int,\n",
    "            dk_arr  # array-like\n",
    "    ):\n",
    "        num_feat = x.shape[1]\n",
    "        assert w.shape[0] == num_feat + 1, \"w shape is mismatch to x\"\n",
    "        assert len(w.shape) == 1 or w.shape[1] == 1, \"w should be list or 1D array\"\n",
    "        w = w.reshape((w.shape[0], 1))\n",
    "\n",
    "        compute_dk = False\n",
    "        if dk_arr is None:\n",
    "            compute_dk = True\n",
    "            dk_arr = []\n",
    "\n",
    "        for idx in range(total_batch):\n",
    "            begin = idx * batch_size\n",
    "            end = min((idx + 1) * batch_size, x.shape[0])\n",
    "            rows = end - begin\n",
    "            # padding one col for bias in w\n",
    "            x_slice = jnp.concatenate(\n",
    "                (x[begin:end, :], jnp.ones((rows, 1))), axis=1\n",
    "            )\n",
    "            y_slice = y[begin:end, :]\n",
    "\n",
    "            pred = jnp.matmul(x_slice, w)\n",
    "            pred = sigmoid(pred, method=self._sig_type)\n",
    "\n",
    "            err = pred - y_slice\n",
    "            grad = jnp.matmul(jnp.transpose(x_slice), err) / rows\n",
    "\n",
    "            if compute_dk:\n",
    "                dk = compute_dk_func(grad, self._eps, self._dk_method)\n",
    "                dk_arr.append(dk)\n",
    "            else:\n",
    "                dk = dk_arr[idx]\n",
    "\n",
    "            step = self._learning_rate * grad * dk\n",
    "\n",
    "            w = w - step\n",
    "\n",
    "        if compute_dk:\n",
    "            dk_arr = jnp.array(dk_arr)\n",
    "\n",
    "        return w, dk_arr\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fit LR with policy-sgd.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        y : ndarray of shape (n_samples, 1)\n",
    "            Target values.\n",
    "\n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 2, f\"expect x to be 2 dimension array, got {x.shape}\"\n",
    "        assert len(y.shape) == 2, f\"expect y to be 2 dimension array, got {y.shape}\"\n",
    "\n",
    "        num_sample = x.shape[0]\n",
    "        num_feat = x.shape[1]\n",
    "        batch_size = min(self._batch_size, num_sample)\n",
    "        total_batch = (num_sample + batch_size - 1) // batch_size\n",
    "\n",
    "        # always fit intercept\n",
    "        weights = jnp.zeros((num_feat + 1, 1))\n",
    "        dk_arr = None\n",
    "\n",
    "        # do train\n",
    "        for _ in range(self._epochs):\n",
    "            weights, dk_arr = self._update_weights(\n",
    "                x,\n",
    "                y,\n",
    "                weights,\n",
    "                total_batch,\n",
    "                batch_size,\n",
    "                dk_arr\n",
    "            )\n",
    "\n",
    "        self._weights = weights\n",
    "        self.dk_arr = dk_arr\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"Probability estimates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape (n_samples, n_features)\n",
    "            Input data for prediction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples, n_classes)\n",
    "            Returns the probability of the sample for each class in the model,\n",
    "            where classes are ordered as they are in `self.classes_`.\n",
    "        \"\"\"\n",
    "        num_feat = x.shape[1]\n",
    "        w = self._weights\n",
    "        assert w.shape[0] == num_feat + 1, f\"w shape is mismatch to x={x.shape}\"\n",
    "        assert len(w.shape) == 1 or w.shape[1] == 1, \"w should be list or 1D array\"\n",
    "        w.reshape((w.shape[0], 1))\n",
    "\n",
    "        bias = w[-1, 0]\n",
    "        w = jnp.resize(w, (num_feat, 1))\n",
    "\n",
    "        pred = jnp.matmul(x, w) + bias\n",
    "        pred = sigmoid(pred, method=self._sig_type)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, let's try this algorithm in plaintext!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "plain_model = SSLRSGDClassifier(\n",
    "    epochs=3,\n",
    "    learning_rate=0.1,\n",
    "    batch_size=8,\n",
    "    sig_type='t1',\n",
    "    eps=1e-6,\n",
    "    dk_method='norm'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plain_model.fit(X.values, y.values.reshape(-1, 1))  # X, y should be two-dimension array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Things seem go well, try to predict the dataset and compute auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_prob = plain_model.predict_proba(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9903083875059459"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y.values, predict_prob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 2: Run algorithm with simulator\n",
    "\n",
    "Normally, you can just do something like [LR with spu](https://www.secretflow.org.cn/docs/secretflow/en/tutorial/lr_with_spu.html) to run your program within a secure context: move you dataset to PYU or SPU, run program with SPU you declare and reveal some information you need(`reveal` is a **very dangerous** op, and you should use it very carefully in real application).\n",
    "\n",
    "However, we will see later that you may come across large **metric gap**(like auc in LR) between plaintext and secret. It will be a better choice that developer can run MPC program simpler with high flexibility to adjust hyper-parameters like the size of ring, fxp or underlying MPC protocol etc.\n",
    "\n",
    "So in this part, we will show how to use simulator to run our algorithm just like running normal MPC program, and do minimum experiments to focus and verify the pitfall of the program.\n",
    "To use simulator but not running program with SPU Device directly has two advantages:\n",
    "\n",
    "1. **Fewer Code**: No need to deal with tons of `DeviceObject` and move data from PYU between SPU.\n",
    "2. **Quicker Experiment**: No ray cluster connected, run experiments end-to-end.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spu.utils.simulation as spsim\n",
    "import spu.spu_pb2 as spu_pb2\n",
    "import spu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, to simulate , we first define a simple simulator with CHEETAH protocol and 64 bits ring in 2pc settings. We will talk about 3pc later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim = spsim.Simulator.simple(\n",
    "    2, spu_pb2.ProtocolKind.CHEETAH, spu_pb2.FieldType.FM64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_and_predict(x, y, epochs=3, learning_rate=0.1, batch_size=8, sig_type='t1', eps=1e-6, dk_method='norm'):\n",
    "    model = SSLRSGDClassifier(\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        sig_type=sig_type,\n",
    "        eps=eps,\n",
    "        dk_method=dk_method\n",
    "    )\n",
    "    model.fit(x, y)\n",
    "    return model.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-27 15:42:55.514] [info] [cheetah_dot.cc:328] CheetahDot uses 3@2 modulus 8192 degree for 64 bit ring\n",
      "[2023-03-27 15:42:55.520] [info] [cheetah_dot.cc:328] CheetahDot uses 3@2 modulus 8192 degree for 64 bit ring\n",
      "[2023-03-27 15:42:55.547] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-27 15:42:55.547] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-27 15:42:55.612] [info] [thread_pool.cc:30] Create a fixed thread pool with size 63\n"
     ]
    }
   ],
   "source": [
    "result = spsim.sim_jax(sim, fit_and_predict)(X.values, y.values.reshape(-1, 1))  # X, y should be two-dimension array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49056603773584906"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, result)  # rather pool under cheetah protocol!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then, we try it in 3pc setting, i.e. use ABY3 protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim_aby = spsim.Simulator.simple(\n",
    "    3, spu_pb2.ProtocolKind.ABY3, spu_pb2.FieldType.FM64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = spsim.sim_jax(sim_aby, fit_and_predict)(X.values, y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.970865704772475"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9903083875059457"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not very stable, if you run the fit procedure multiple times, you will sometimes get 0.97\n",
    "result = spsim.sim_jax(sim_aby, fit_and_predict)(X.values, y.values.reshape(-1, 1))\n",
    "roc_auc_score(y, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When the program runs in secret without any modification, the auc may drop dramatically after training 3 epochs(from 0.990 to 0.490 for cheetah)!\n",
    "\n",
    "We will give some analysis and try to fix it first from application perspective and think deeper in MPC perspective.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Application Perspective\n",
    "Before we dive into this question, we can first summarize the differences of policy-sgd between naive-sgd are:\n",
    "\n",
    "1. Using approximation function to compute sigmoid(default is t1).  \n",
    "\n",
    "2. The scale of learning rate, which contains the computation of dk as defined in `compute_dk_func`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Doing some simple math, we can notice that t1 approximation will force the pred to 0 when inner product is less than -2 and to 1 when inner product is large than 2. So when we compute gradient with:\n",
    "$$ grad = \\frac{1}{n} \\sum_{i} (sigmoid(w^T x_i) - y_i) x_i $$\n",
    "If coincidentally, we can get all elements of grad very near to 0(may have little error in MPC), then the `dk` computed in first epoch becomes very large, and may result in the failure of training. We can verify this by simply enlarge the `batch_size` to 64 which can decrease the probability of all-zero problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-07 15:07:27.750] [info] [cheetah_dot.cc:328] CheetahDot uses 3@2 modulus 8192 degree for 64 bit ring\n",
      "[2023-03-07 15:07:27.753] [info] [cheetah_dot.cc:328] CheetahDot uses 3@2 modulus 8192 degree for 64 bit ring\n",
      "[2023-03-07 15:07:27.787] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-07 15:07:27.787] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n"
     ]
    }
   ],
   "source": [
    "# use partial to fix batch_size=64\n",
    "result = spsim.sim_jax(sim, partial(fit_and_predict, batch_size=64))(X.values, y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9892711801701812"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Restricting large batch_size is not an appropriate way, the key is to make the scale factor smaller, we can also fix the question by enlarging the `eps`, e.g. change `eps` from 1e-6 to 1e-2.\n",
    "\n",
    "\n",
    "**Note**: `eps` in policy-sgd indeed has two affects, one is to prevent the zero-division error, the other is to restrict the maximum scale factor in warm-start phase(first epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-07 15:07:39.660] [info] [cheetah_dot.cc:328] CheetahDot uses 3@2 modulus 8192 degree for 64 bit ring\n",
      "[2023-03-07 15:07:39.660] [info] [cheetah_dot.cc:328] CheetahDot uses 3@2 modulus 8192 degree for 64 bit ring\n",
      "[2023-03-07 15:07:39.695] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-07 15:07:39.695] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n"
     ]
    }
   ],
   "source": [
    "result = spsim.sim_jax(sim, partial(fit_and_predict, eps=1e-2))(X.values, y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9884387717351092"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The above analyses are based on t1 sigmoid, which leads to 0 in grad. So we can switch the t1 approximation to other non-truncate but costly form(e.g. sr approximation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-22 11:19:55.316] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-22 11:19:55.316] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-22 11:19:55.450] [info] [cheetah_dot.cc:328] CheetahDot uses 3@2 modulus 8192 degree for 64 bit ring\n",
      "[2023-03-22 11:19:55.450] [info] [cheetah_dot.cc:328] CheetahDot uses 3@2 modulus 8192 degree for 64 bit ring\n"
     ]
    }
   ],
   "source": [
    "result = spsim.sim_jax(sim, partial(fit_and_predict, sig_type='sr'))(X.values, y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9921647904444797"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, if we just consider it on app layer, we can get three rules for fixing:\n",
    "\n",
    "1. enlarge `batch_size`.\n",
    "\n",
    "2. enlarge `eps`.\n",
    "\n",
    "3. use non-truncate sigmoid approximation(e.g. sr approximation).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MPC Perspective\n",
    "\n",
    "In this part, we concentrate more on why huge error occurs. To achieve this goal, we will talk according to underlying protocol and use simulator to do some **experiments** to confirm our hypothesis. Readers can do the similar things when you develop your own secure application.\n",
    "\n",
    "Before diving into the problem deeper, we highly recommend the reader to read: \n",
    "\n",
    " 1. [spu_inside](https://www.secretflow.org.cn/docs/spu/en/getting_started/tutorials/spu_inside.html#Tracing): gives some introductions how spu works inside for float-point operations.\n",
    "\n",
    " 2. [pitfall](https://www.secretflow.org.cn/docs/spu/en/reference/fxp.html): spu implements math function(like `reciprocal`, `log` and so on) with approximation algorithm, so some precision issue will occur when inputs fall into some intervals. We list some known issue about this.\n",
    "\n",
    "3. [protocols](https://www.secretflow.org.cn/docs/spu/en/reference/mpc_status.html): list all protocols spu implements now. Generally speaking, for 2pc, it's safe to use cheetah, while for 3pc, ABY3 is the only choice.\n",
    "\n",
    "First define a function just like `fit_and_predict` to get dk_arr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dk(x, y, epochs=3, learning_rate=0.1, batch_size=8, sig_type='t1', eps=1e-6, dk_method='norm'):\n",
    "    model = SSLRSGDClassifier(\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        sig_type=sig_type,\n",
    "        eps=eps,\n",
    "        dk_method=dk_method\n",
    "    )\n",
    "    model.fit(x, y)\n",
    "    return model.dk_arr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2PC: Cheetah Protocol\n",
    "\n",
    "Recap:\n",
    "\n",
    "1. [cheetah](https://eprint.iacr.org/2022/207) is a fast 2pc semi-honest protocol which uses FHE to accelerate the computation. But it will have 0-2 bits error when do `mul` or `dot`.\n",
    "\n",
    "2. If 64-bits ring, about 18 bitwidth fixed-point number will be used. So the minimum positive float spu can represent is $\\frac{1}{2^{18}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We first check the output of dk_arr and try to find the caveat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-21 20:05:05.795] [info] [cheetah_dot.cc:328] CheetahDot uses 3@2 modulus 8192 degree for 64 bit ring\n",
      "[2023-03-21 20:05:05.795] [info] [cheetah_dot.cc:328] CheetahDot uses 3@2 modulus 8192 degree for 64 bit ring\n",
      "[2023-03-21 20:05:05.834] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-21 20:05:05.834] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n"
     ]
    }
   ],
   "source": [
    "result = spsim.sim_jax(sim, get_dk)(X.values, y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Surprisingly, we get a very **small negative number** which makes that weight update wrong!(the opposite direction and large scale factor for sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-131072.25"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However, if we use a bigger ring, then everything is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a simulator with 128 rings\n",
    "sim128 = spsim.Simulator.simple(\n",
    "    2, spu_pb2.ProtocolKind.CHEETAH, spu_pb2.FieldType.FM128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-07 15:43:05.702] [info] [cheetah_dot.cc:328] CheetahDot uses 5@3 modulus 16384 degree for 128 bit ring\n",
      "[2023-03-07 15:43:05.703] [info] [cheetah_dot.cc:328] CheetahDot uses 5@3 modulus 16384 degree for 128 bit ring\n",
      "[2023-03-07 15:43:05.776] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 8 modulus (36 bit each) for 128 bit ring\n",
      "[2023-03-07 15:43:05.776] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 8 modulus (36 bit each) for 128 bit ring\n"
     ]
    }
   ],
   "source": [
    "result = spsim.sim_jax(sim128, fit_and_predict)(X.values, y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9903083875059457"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y, result)  # auc is just like plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-07 15:44:07.277] [info] [cheetah_dot.cc:328] CheetahDot uses 5@3 modulus 16384 degree for 128 bit ring\n",
      "[2023-03-07 15:44:07.278] [info] [cheetah_dot.cc:328] CheetahDot uses 5@3 modulus 16384 degree for 128 bit ring\n",
      "[2023-03-07 15:44:07.349] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 8 modulus (36 bit each) for 128 bit ring\n",
      "[2023-03-07 15:44:07.349] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 8 modulus (36 bit each) for 128 bit ring\n"
     ]
    }
   ],
   "source": [
    "result = spsim.sim_jax(sim128, get_dk)(X.values, y.values.reshape(-1, 1))[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.03009"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From the above outputs, we can guess if input is near $\\frac{1}{2^{18}}$ and use cheetah protocol, when doing `square` and `sum`, the bit error may be significant and not negligible(`mul` and `dot` have 0-2 bit errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's test this\n",
    "def test_square_and_sum_when_x_small(x):\n",
    "    return jnp.sum(jnp.square(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-21 20:12:09.710] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-21 20:12:09.711] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(-3.8146973e-05, dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spsim.sim_jax(sim, test_square_and_sum_when_x_small)(np.array([1e-5] * 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_norm_when_x_small(x):\n",
    "    return jnp.sqrt(jnp.sum(jnp.square(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-27 15:50:34.921] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-27 15:50:34.922] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(-7.6293945e-06, dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spsim.sim_jax(sim, test_norm_when_x_small)(\n",
    "    np.array([1e-5] * 10))  # for small input, sqrt just output very small number!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So if `eps=1e-6`, the two norm of grad may be a negative number when each element of grad is near to $\\frac{1}{2^{18}}$, then we get a very small negative dk.\n",
    "\n",
    "This explains the claims we get from app layer:  \n",
    "1. Enlarging `batch_size`: the probability of all elements of grad is zero becomes small.\n",
    "\n",
    "2. Enlarging `eps`: force the denominator to be positive number."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3PC: ABY3 Protocol\n",
    "\n",
    "We still check the dk_arr first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = spsim.sim_jax(sim_aby, get_dk)(X.values, y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "emmmmmm, strange value occurs again, we find **0** in dk_arr!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[66]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Comparing with small negative number, 0 is a mild error for our update procedure. It just does nothing in that iter, so the final auc may drop a little(from 0.99 to 0.97, users can test yourself that if you set eps to 1e-2, then the result will be very stable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Likewise, We always check the computation of 2-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0., dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spsim.sim_jax(sim_aby, test_norm_when_x_small)(np.array([1e-5] * 10))  # get 0 is acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3.1622774e-05, dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_norm_when_x_small(np.array([1e-5] * 10))  # 2-norm in plaintext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then, check the reciprocal op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_reciprocal_when_x_small(x):\n",
    "    return 1 / (x + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 0 when denominator very small, which is the caveat of reciprocal!\n",
    "spsim.sim_jax(sim_aby, test_reciprocal_when_x_small)(np.array([0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Something More\n",
    "\n",
    "Indeed, there are some other interesting things in SSLR. Here, due to length limitations, we just give some hints, and readers can do more simulations yourself!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Rsqrt v.s. Norm\n",
    "\n",
    "We can recall that the `compute_dk_func` function defined in Part 1 contains a `method` arg, and we just ignore this arg before. Indeed, we can tell simulator to print more information like [spu_inside](https://www.secretflow.org.cn/docs/spu/en/getting_started/tutorials/spu_inside.html#Tracing) do: enable **hlo**(High Level Operations) trace and profile on. Then we can figure out which op has been invoked and its time cost.\n",
    "\n",
    "Here, we list some advantages of using `jax.lax.rsqrt` rather than `jnp.linalg.norm`:\n",
    "\n",
    "1. Fewer bytes and few send actions: which leads to smaller running time(See the following comments and notes for details).\n",
    "\n",
    "2. More stable when given same `eps`: if we regard `f(x)` as `compute_dk_func` with `method=norm`, and `g(x)` with `method=rsqrt`, then the users can do simulation yourself, and find `f(x)` has higher relative error than `g(x)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we define a cheetah config with pphlo trace and profile on\n",
    "config_che = spu.RuntimeConfig(\n",
    "    protocol=spu_pb2.ProtocolKind.CHEETAH,\n",
    "    field=spu.FieldType.FM64,\n",
    "    fxp_fraction_bits=18,\n",
    "    enable_pphlo_trace=True,\n",
    "    enable_pphlo_profile=True\n",
    ")\n",
    "simulator_che = spsim.Simulator(2, config_che)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-27 16:41:14.168] [info] [pphlo_executor.cc:1107] PPHLO %0 = \"pphlo.constant\"() {value = dense<9.99999997E-7> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 16:41:14.168] [info] [pphlo_executor.cc:1107] PPHLO %0 = \"pphlo.constant\"() {value = dense<9.99999997E-7> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 16:41:14.168] [info] [pphlo_executor.cc:1107] PPHLO %1 = \"pphlo.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 16:41:14.168] [info] [pphlo_executor.cc:1107] PPHLO %1 = \"pphlo.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 16:41:14.168] [info] [pphlo_executor.cc:1107] PPHLO %2 = \"pphlo.constant\"() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 16:41:14.168] [info] [pphlo_executor.cc:1107] PPHLO %3 = \"pphlo.multiply\"(%arg0, %arg0) : (tensor<1000x!pphlo.sec<f32>>, tensor<1000x!pphlo.sec<f32>>) -> tensor<1000x!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:14.168] [info] [pphlo_executor.cc:1107] PPHLO %2 = \"pphlo.constant\"() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 16:41:14.168] [info] [pphlo_executor.cc:1107] PPHLO %3 = \"pphlo.multiply\"(%arg0, %arg0) : (tensor<1000x!pphlo.sec<f32>>, tensor<1000x!pphlo.sec<f32>>) -> tensor<1000x!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:14.168] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-27 16:41:14.168] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-27 16:41:15.782] [info] [pphlo_executor.cc:1107] PPHLO %4 = \"pphlo.convert\"(%1) : (tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:15.782] [info] [pphlo_executor.cc:1107] PPHLO %5 = \"pphlo.reduce\"(%3, %4) ({\n",
      "^bb0(%arg1: tensor<!pphlo.sec<f32>>, %arg2: tensor<!pphlo.sec<f32>>):\n",
      "  %9 = \"pphlo.add\"(%arg1, %arg2) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "  \"pphlo.return\"(%9) : (tensor<!pphlo.sec<f32>>) -> ()\n",
      "}) {dimensions = dense<0> : tensor<1xi64>} : (tensor<1000x!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:15.782] [info] [pphlo_executor.cc:1107] PPHLO %4 = \"pphlo.convert\"(%1) : (tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:15.782] [info] [pphlo_executor.cc:1107] PPHLO %5 = \"pphlo.reduce\"(%3, %4) ({\n",
      "^bb0(%arg1: tensor<!pphlo.sec<f32>>, %arg2: tensor<!pphlo.sec<f32>>):\n",
      "  %9 = \"pphlo.add\"(%arg1, %arg2) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "  \"pphlo.return\"(%9) : (tensor<!pphlo.sec<f32>>) -> ()\n",
      "}) {dimensions = dense<0> : tensor<1xi64>} : (tensor<1000x!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:15.782] [info] [pphlo_executor.cc:1107] PPHLO %6 = \"pphlo.sqrt\"(%5) : (tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:15.783] [info] [pphlo_executor.cc:1107] PPHLO %6 = \"pphlo.sqrt\"(%5) : (tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:15.791] [info] [pphlo_executor.cc:1107] PPHLO %7 = \"pphlo.add\"(%6, %0) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:15.791] [info] [pphlo_executor.cc:1107] PPHLO %7 = \"pphlo.add\"(%6, %0) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:15.791] [info] [pphlo_executor.cc:1107] PPHLO %8 = \"pphlo.divide\"(%2, %7) : (tensor<!pphlo.pub<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:15.791] [info] [pphlo_executor.cc:1107] PPHLO %8 = \"pphlo.divide\"(%2, %7) : (tensor<!pphlo.pub<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:15.794] [info] [api.cc:131] [Profiling] SPU execution compute_dk_func completed, input processing took 1.66e-06s, execution took 1.628297569s, output processing took 2.143e-06s, total time 1.628301372s.\n",
      "[2023-03-27 16:41:15.794] [info] [api.cc:163] HLO profiling: total time 1.6252017869999997\n",
      "[2023-03-27 16:41:15.794] [info] [api.cc:166] - pphlo.add, executed 1 times, duration 8.887e-06s\n",
      "[2023-03-27 16:41:15.794] [info] [api.cc:166] - pphlo.constant, executed 3 times, duration 4.6888e-05s\n",
      "[2023-03-27 16:41:15.794] [info] [api.cc:166] - pphlo.convert, executed 1 times, duration 2.9952e-05s\n",
      "[2023-03-27 16:41:15.794] [info] [api.cc:166] - pphlo.divide, executed 1 times, duration 0.002838882s\n",
      "[2023-03-27 16:41:15.794] [info] [api.cc:166] - pphlo.multiply, executed 1 times, duration 1.613989704s\n",
      "[2023-03-27 16:41:15.794] [info] [api.cc:166] - pphlo.reduce, executed 1 times, duration 0.00016315s\n",
      "[2023-03-27 16:41:15.794] [info] [api.cc:166] - pphlo.sqrt, executed 1 times, duration 0.008124324s\n",
      "[2023-03-27 16:41:15.794] [info] [api.cc:175] Link details: total send bytes 1479115, send actions 203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.05480957, dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spsim.sim_jax(simulator_che, partial(compute_dk_func, method='norm'))(np.arange(1000) / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If directly invoking rsqrt, you can find send actions have obvious drop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-27 16:41:21.767] [info] [pphlo_executor.cc:1107] PPHLO %0 = \"pphlo.constant\"() {value = dense<9.99999997E-7> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 16:41:21.767] [info] [pphlo_executor.cc:1107] PPHLO %0 = \"pphlo.constant\"() {value = dense<9.99999997E-7> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 16:41:21.768] [info] [pphlo_executor.cc:1107] PPHLO %1 = \"pphlo.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 16:41:21.768] [info] [pphlo_executor.cc:1107] PPHLO %1 = \"pphlo.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 16:41:21.768] [info] [pphlo_executor.cc:1107] PPHLO %2 = \"pphlo.multiply\"(%arg0, %arg0) : (tensor<1000x!pphlo.sec<f32>>, tensor<1000x!pphlo.sec<f32>>) -> tensor<1000x!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:21.768] [info] [pphlo_executor.cc:1107] PPHLO %2 = \"pphlo.multiply\"(%arg0, %arg0) : (tensor<1000x!pphlo.sec<f32>>, tensor<1000x!pphlo.sec<f32>>) -> tensor<1000x!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:21.768] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-27 16:41:21.768] [info] [cheetah_mul.cc:290] BeaverCheetah::Mul uses 4 modulus (36 bit each) for 64 bit ring\n",
      "[2023-03-27 16:41:23.241] [info] [pphlo_executor.cc:1107] PPHLO %3 = \"pphlo.convert\"(%1) : (tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:23.241] [info] [pphlo_executor.cc:1107] PPHLO %4 = \"pphlo.reduce\"(%2, %3) ({\n",
      "^bb0(%arg1: tensor<!pphlo.sec<f32>>, %arg2: tensor<!pphlo.sec<f32>>):\n",
      "  %7 = \"pphlo.add\"(%arg1, %arg2) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "  \"pphlo.return\"(%7) : (tensor<!pphlo.sec<f32>>) -> ()\n",
      "}) {dimensions = dense<0> : tensor<1xi64>} : (tensor<1000x!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:23.241] [info] [pphlo_executor.cc:1107] PPHLO %3 = \"pphlo.convert\"(%1) : (tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:23.241] [info] [pphlo_executor.cc:1107] PPHLO %5 = \"pphlo.add\"(%4, %0) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:23.241] [info] [pphlo_executor.cc:1107] PPHLO %4 = \"pphlo.reduce\"(%2, %3) ({\n",
      "^bb0(%arg1: tensor<!pphlo.sec<f32>>, %arg2: tensor<!pphlo.sec<f32>>):\n",
      "  %7 = \"pphlo.add\"(%arg1, %arg2) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "  \"pphlo.return\"(%7) : (tensor<!pphlo.sec<f32>>) -> ()\n",
      "}) {dimensions = dense<0> : tensor<1xi64>} : (tensor<1000x!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:23.241] [info] [pphlo_executor.cc:1107] PPHLO %6 = \"pphlo.rsqrt\"(%5) : (tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:23.242] [info] [pphlo_executor.cc:1107] PPHLO %5 = \"pphlo.add\"(%4, %0) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:23.242] [info] [pphlo_executor.cc:1107] PPHLO %6 = \"pphlo.rsqrt\"(%5) : (tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 16:41:23.247] [info] [api.cc:131] [Profiling] SPU execution compute_dk_func completed, input processing took 1.495e-06s, execution took 1.481584644s, output processing took 1.772e-06s, total time 1.481587911s.\n",
      "[2023-03-27 16:41:23.247] [info] [api.cc:163] HLO profiling: total time 1.478751855\n",
      "[2023-03-27 16:41:23.247] [info] [api.cc:166] - pphlo.add, executed 1 times, duration 8.05e-06s\n",
      "[2023-03-27 16:41:23.247] [info] [api.cc:166] - pphlo.constant, executed 2 times, duration 2.857e-05s\n",
      "[2023-03-27 16:41:23.247] [info] [api.cc:166] - pphlo.convert, executed 1 times, duration 3.5875e-05s\n",
      "[2023-03-27 16:41:23.247] [info] [api.cc:166] - pphlo.multiply, executed 1 times, duration 1.472898415s\n",
      "[2023-03-27 16:41:23.247] [info] [api.cc:166] - pphlo.reduce, executed 1 times, duration 0.000173326s\n",
      "[2023-03-27 16:41:23.247] [info] [api.cc:166] - pphlo.rsqrt, executed 1 times, duration 0.005607619s\n",
      "[2023-03-27 16:41:23.247] [info] [api.cc:175] Link details: total send bytes 1474898, send actions 89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.05480957, dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note:\n",
    "# 1. the total time cost by rsqrt may be even larger than norm, the reason of this is that CHEETAH use FHE, so the cost of multiply is very huge comparing to other ops.\n",
    "# 2. time(rsqrt) = 0.005607 < time(sqrt+divide) = 0.00812 + 0.00283\n",
    "spsim.sim_jax(simulator_che, partial(compute_dk_func, method='rsqrt'))(np.arange(1000) / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Also, we can define an aby3 config with pphlo trace and profile on\n",
    "config_aby = spu.RuntimeConfig(\n",
    "    protocol=spu_pb2.ProtocolKind.ABY3,\n",
    "    field=spu.FieldType.FM64,\n",
    "    fxp_fraction_bits=18,\n",
    "    enable_pphlo_trace=True,\n",
    "    enable_pphlo_profile=True\n",
    ")\n",
    "simulator_aby = spsim.Simulator(3, config_aby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %0 = \"pphlo.constant\"() {value = dense<9.99999997E-7> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %0 = \"pphlo.constant\"() {value = dense<9.99999997E-7> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %1 = \"pphlo.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %1 = \"pphlo.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %0 = \"pphlo.constant\"() {value = dense<9.99999997E-7> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %2 = \"pphlo.constant\"() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %2 = \"pphlo.constant\"() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %1 = \"pphlo.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %3 = \"pphlo.multiply\"(%arg0, %arg0) : (tensor<1000x!pphlo.sec<f32>>, tensor<1000x!pphlo.sec<f32>>) -> tensor<1000x!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %3 = \"pphlo.multiply\"(%arg0, %arg0) : (tensor<1000x!pphlo.sec<f32>>, tensor<1000x!pphlo.sec<f32>>) -> tensor<1000x!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %2 = \"pphlo.constant\"() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %3 = \"pphlo.multiply\"(%arg0, %arg0) : (tensor<1000x!pphlo.sec<f32>>, tensor<1000x!pphlo.sec<f32>>) -> tensor<1000x!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %4 = \"pphlo.convert\"(%1) : (tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %4 = \"pphlo.convert\"(%1) : (tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.735] [info] [pphlo_executor.cc:1107] PPHLO %4 = \"pphlo.convert\"(%1) : (tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.736] [info] [pphlo_executor.cc:1107] PPHLO %5 = \"pphlo.reduce\"(%3, %4) ({\n",
      "^bb0(%arg1: tensor<!pphlo.sec<f32>>, %arg2: tensor<!pphlo.sec<f32>>):\n",
      "  %9 = \"pphlo.add\"(%arg1, %arg2) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "  \"pphlo.return\"(%9) : (tensor<!pphlo.sec<f32>>) -> ()\n",
      "}) {dimensions = dense<0> : tensor<1xi64>} : (tensor<1000x!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.736] [info] [pphlo_executor.cc:1107] PPHLO %5 = \"pphlo.reduce\"(%3, %4) ({\n",
      "^bb0(%arg1: tensor<!pphlo.sec<f32>>, %arg2: tensor<!pphlo.sec<f32>>):\n",
      "  %9 = \"pphlo.add\"(%arg1, %arg2) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "  \"pphlo.return\"(%9) : (tensor<!pphlo.sec<f32>>) -> ()\n",
      "}) {dimensions = dense<0> : tensor<1xi64>} : (tensor<1000x!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.736] [info] [pphlo_executor.cc:1107] PPHLO %5 = \"pphlo.reduce\"(%3, %4) ({\n",
      "^bb0(%arg1: tensor<!pphlo.sec<f32>>, %arg2: tensor<!pphlo.sec<f32>>):\n",
      "  %9 = \"pphlo.add\"(%arg1, %arg2) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "  \"pphlo.return\"(%9) : (tensor<!pphlo.sec<f32>>) -> ()\n",
      "}) {dimensions = dense<0> : tensor<1xi64>} : (tensor<1000x!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.736] [info] [pphlo_executor.cc:1107] PPHLO %6 = \"pphlo.sqrt\"(%5) : (tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.736] [info] [pphlo_executor.cc:1107] PPHLO %6 = \"pphlo.sqrt\"(%5) : (tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.736] [info] [pphlo_executor.cc:1107] PPHLO %6 = \"pphlo.sqrt\"(%5) : (tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.740] [info] [pphlo_executor.cc:1107] PPHLO %7 = \"pphlo.add\"(%6, %0) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.740] [info] [pphlo_executor.cc:1107] PPHLO %7 = \"pphlo.add\"(%6, %0) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.740] [info] [pphlo_executor.cc:1107] PPHLO %7 = \"pphlo.add\"(%6, %0) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.740] [info] [pphlo_executor.cc:1107] PPHLO %8 = \"pphlo.divide\"(%2, %7) : (tensor<!pphlo.pub<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.740] [info] [pphlo_executor.cc:1107] PPHLO %8 = \"pphlo.divide\"(%2, %7) : (tensor<!pphlo.pub<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.740] [info] [pphlo_executor.cc:1107] PPHLO %8 = \"pphlo.divide\"(%2, %7) : (tensor<!pphlo.pub<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:22.743] [info] [api.cc:131] [Profiling] SPU execution compute_dk_func completed, input processing took 1.842e-06s, execution took 0.010443034s, output processing took 2.523e-06s, total time 0.010447399s.\n",
      "[2023-03-27 15:51:22.743] [info] [api.cc:163] HLO profiling: total time 0.0073849860000000005\n",
      "[2023-03-27 15:51:22.743] [info] [api.cc:166] - pphlo.add, executed 1 times, duration 1.6082e-05s\n",
      "[2023-03-27 15:51:22.743] [info] [api.cc:166] - pphlo.constant, executed 3 times, duration 4.971e-05s\n",
      "[2023-03-27 15:51:22.743] [info] [api.cc:166] - pphlo.convert, executed 1 times, duration 7.6821e-05s\n",
      "[2023-03-27 15:51:22.743] [info] [api.cc:166] - pphlo.divide, executed 1 times, duration 0.002891194s\n",
      "[2023-03-27 15:51:22.743] [info] [api.cc:166] - pphlo.multiply, executed 1 times, duration 0.000235304s\n",
      "[2023-03-27 15:51:22.743] [info] [api.cc:166] - pphlo.reduce, executed 1 times, duration 0.000514333s\n",
      "[2023-03-27 15:51:22.743] [info] [api.cc:166] - pphlo.sqrt, executed 1 times, duration 0.003601542s\n",
      "[2023-03-27 15:51:22.743] [info] [api.cc:175] Link details: total send bytes 41668, send actions 149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.05481339, dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spsim.sim_jax(simulator_aby, partial(compute_dk_func, method='norm'))(np.arange(1000) / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When using aby3, you can find both the send actions and send bytes drop large if using rsqrt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %0 = \"pphlo.constant\"() {value = dense<9.99999997E-7> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %0 = \"pphlo.constant\"() {value = dense<9.99999997E-7> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %0 = \"pphlo.constant\"() {value = dense<9.99999997E-7> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %1 = \"pphlo.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %1 = \"pphlo.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %1 = \"pphlo.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<!pphlo.pub<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %2 = \"pphlo.multiply\"(%arg0, %arg0) : (tensor<1000x!pphlo.sec<f32>>, tensor<1000x!pphlo.sec<f32>>) -> tensor<1000x!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %2 = \"pphlo.multiply\"(%arg0, %arg0) : (tensor<1000x!pphlo.sec<f32>>, tensor<1000x!pphlo.sec<f32>>) -> tensor<1000x!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %2 = \"pphlo.multiply\"(%arg0, %arg0) : (tensor<1000x!pphlo.sec<f32>>, tensor<1000x!pphlo.sec<f32>>) -> tensor<1000x!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %3 = \"pphlo.convert\"(%1) : (tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %3 = \"pphlo.convert\"(%1) : (tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %3 = \"pphlo.convert\"(%1) : (tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %4 = \"pphlo.reduce\"(%2, %3) ({\n",
      "^bb0(%arg1: tensor<!pphlo.sec<f32>>, %arg2: tensor<!pphlo.sec<f32>>):\n",
      "  %7 = \"pphlo.add\"(%arg1, %arg2) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "  \"pphlo.return\"(%7) : (tensor<!pphlo.sec<f32>>) -> ()\n",
      "}) {dimensions = dense<0> : tensor<1xi64>} : (tensor<1000x!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %4 = \"pphlo.reduce\"(%2, %3) ({\n",
      "^bb0(%arg1: tensor<!pphlo.sec<f32>>, %arg2: tensor<!pphlo.sec<f32>>):\n",
      "  %7 = \"pphlo.add\"(%arg1, %arg2) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "  \"pphlo.return\"(%7) : (tensor<!pphlo.sec<f32>>) -> ()\n",
      "}) {dimensions = dense<0> : tensor<1xi64>} : (tensor<1000x!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.671] [info] [pphlo_executor.cc:1107] PPHLO %4 = \"pphlo.reduce\"(%2, %3) ({\n",
      "^bb0(%arg1: tensor<!pphlo.sec<f32>>, %arg2: tensor<!pphlo.sec<f32>>):\n",
      "  %7 = \"pphlo.add\"(%arg1, %arg2) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "  \"pphlo.return\"(%7) : (tensor<!pphlo.sec<f32>>) -> ()\n",
      "}) {dimensions = dense<0> : tensor<1xi64>} : (tensor<1000x!pphlo.sec<f32>>, tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.672] [info] [pphlo_executor.cc:1107] PPHLO %5 = \"pphlo.add\"(%4, %0) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.672] [info] [pphlo_executor.cc:1107] PPHLO %5 = \"pphlo.add\"(%4, %0) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.672] [info] [pphlo_executor.cc:1107] PPHLO %5 = \"pphlo.add\"(%4, %0) : (tensor<!pphlo.sec<f32>>, tensor<!pphlo.pub<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.672] [info] [pphlo_executor.cc:1107] PPHLO %6 = \"pphlo.rsqrt\"(%5) : (tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.672] [info] [pphlo_executor.cc:1107] PPHLO %6 = \"pphlo.rsqrt\"(%5) : (tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.672] [info] [pphlo_executor.cc:1107] PPHLO %6 = \"pphlo.rsqrt\"(%5) : (tensor<!pphlo.sec<f32>>) -> tensor<!pphlo.sec<f32>>\n",
      "[2023-03-27 15:51:25.676] [info] [api.cc:131] [Profiling] SPU execution compute_dk_func completed, input processing took 1.92e-06s, execution took 0.006332778s, output processing took 1.933e-06s, total time 0.006336631s.\n",
      "[2023-03-27 15:51:25.676] [info] [api.cc:163] HLO profiling: total time 0.003986974\n",
      "[2023-03-27 15:51:25.676] [info] [api.cc:166] - pphlo.add, executed 1 times, duration 2.5793e-05s\n",
      "[2023-03-27 15:51:25.676] [info] [api.cc:166] - pphlo.constant, executed 2 times, duration 2.5628e-05s\n",
      "[2023-03-27 15:51:25.676] [info] [api.cc:166] - pphlo.convert, executed 1 times, duration 3.6405e-05s\n",
      "[2023-03-27 15:51:25.676] [info] [api.cc:166] - pphlo.multiply, executed 1 times, duration 0.000298557s\n",
      "[2023-03-27 15:51:25.676] [info] [api.cc:166] - pphlo.reduce, executed 1 times, duration 0.00050405s\n",
      "[2023-03-27 15:51:25.676] [info] [api.cc:166] - pphlo.rsqrt, executed 1 times, duration 0.003096541s\n",
      "[2023-03-27 15:51:25.676] [info] [api.cc:175] Link details: total send bytes 24704, send actions 63\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.05481339, dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note:\n",
    "# 1. you can find total time of rsqrt will always smaller than norm\n",
    "# 2. likewise, time(rsqrt) = 0.003096 < time(sqrt+divide) = 0.003601 + 0.002891\n",
    "\n",
    "spsim.sim_jax(simulator_aby, partial(compute_dk_func, method='rsqrt'))(np.arange(1000) / 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Computing Loss\n",
    "\n",
    "Many ML frameworks will show validation loss during training procedure when using a validation dataset. It's straight to compute the loss in LR as follows:\n",
    "$$ loss = -\\frac{1}{N} \\sum_{i=1}^N [y_i log(p_i) + (1-y_i) log(1-p_i)] \\quad  (1) $$\n",
    "But when you use t1 approximation for sigmoid, then you may come across $log(0)$ problem. Here, we list two potential recipes to alleviate it.\n",
    "\n",
    "1. **Costly but accurate**: we plug in $p_i = \\frac{1}{1+e^{-w^Tx_i}}$ to (1), then we can get:\n",
    "$$ loss = -\\frac{1}{N} \\sum_{i=1}^N [y_i w^Tx_i - log(1+e^{w^Tx_i})] \\quad  (2) $$\n",
    "this formula solve the $log(0)$ problem, but if $w^Tx_i$ gets too large, as we already know in [pitfall](https://www.secretflow.org.cn/docs/spu/en/reference/fxp.html), this gets **huge errors**! To get stable and accurate formula to compute loss, we notice $log(1+e^{w^T x_i})$ is well-known *Softplus* function, so we can use the equation of Softplus: $log(1+e^{x}) = log(1 + e^{-|x|}) + max(0, x)$, then we can get:\n",
    "$$ loss = -\\frac{1}{N} \\sum_{i=1}^N [y_i w^Tx_i - log(1+e^{-|w^Tx_i|}) - max(w^T x_i, 0)] \\quad (3) $$\n",
    "\n",
    "2. **Cheap but approximate** :Equation (3) can give accurate result, but it contains time-consuming ops($log$, $exp$), which cost a lot! If you just want to compute an approximation of loss(e.g. maybe you want to do early stop with loss), you can try Taylor expansion, which gives:\n",
    "$$ loss = \\frac{1}{N} \\sum_{i=1}^N [log(2) - (y-0.5)w^T x_i + 0.125 * (w^T x_i)^2] $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 3: Run elaborated emulations\n",
    "\n",
    "> Emulations is an **experimental** feature for now, and is under rapid development, so we do not package the code of sml into spu. Users can try this feature from **source code** and run with bazel .Till now, we only have support for LAN setting(`MULTIPROCESS` mode). `Docker` mode, which runs program like under WAN setting, will be posted in future version.\n",
    "\n",
    "Finally, we talk about how to do emulations. Comparing to simulator, emulator runs with a simple scheduler like Secretflow does, and offers some facility(e.g. generate mock data) to make benchmark simpler. So spu provides an `Emulator` class and gives an easy-to-use interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Usually, the emulation will be done with larger dataset, so we won't run directly in this tutorial notebook. Instead, we will show a big picture on how to design and run emulations for MPC application step by step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Setup: define running function\n",
    "\n",
    "Just like what we do in secretflow, we should first define a python function, which will be run in spu. Here, as an example, we just define a very simple function that accepts data from two parties and return the predicted probability after the model trained(you can also split data into training & validation parts, and return the probs of validation dataset.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import sml.utils.emulation as emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import model impl which has been tested with simulator\n",
    "# from Model import model\n",
    "\n",
    "def run_model(model, x1, x2, y):\n",
    "    # here, suppose we divide the dataset into two party\n",
    "    x = jnp.concatenate((x1, x2), axis=1)\n",
    "    y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "    model.fit(x, y)\n",
    "    return model.predict_proba(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Design experiments\n",
    "\n",
    "Taking `SSLRSGDClassifier` as an example, we mainly want to argue that policy-sgd is better than naive-sgd in MPC setting, so we can design the following experiments: \n",
    "\n",
    "1. Find best `dk_method` and `eps` for policy-sgd: for all datasets, compare the accuracy and efficiency.\n",
    "\n",
    "2. Compare the accuracy and efficiency when switching `sig_type` for both policy-sgd and naive-sgd.\n",
    "\n",
    "3. To compare policy-sgd and naive-sgd, we fix `epochs` and test the influence of `learning_rate` and `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for sig_type in ['t1', 'sr']\n",
    "# for dk_method in ['norm', 'rsqrt']\n",
    "# for batch_size in [1024, 2048, 4096]\n",
    "# ...\n",
    "model = SSLRSGDClassifier(epochs=10, learning_rate=0.1, batch_size=2048,\n",
    "                          sig_type='t1', eps=1e-4, dk_method='rsqrt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define running config\n",
    "\n",
    "After designing all the experiments, we can prepare our running config. Currently, we only support `MULTIPROCESS` mode, which uses multiprocess to emulate multi-party and just like running in LAN(`DOCKER` mode which can set `bandwidth` and `latency` to simulate the different WAN settings will be supported in future version).\n",
    "\n",
    "For now, our goal is to compare the accuracy/efficiency diff when switching hyper-param, running program in LAN can be a good choice. Besides, in order to simulate diverse node deployment ways, users can flexibly configure the number of nodes and device situations yourself. You can get some examples of config in `examples/python/conf/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mode = emulation.Mode.MULTIPROCESS  # emulation.Mode.DOCKER for docker not support now\n",
    "# take the mock config as sample, it deploys some nodes in outsourcing way and use ABY3 protocol\n",
    "# Note: in MULTIPROCESS mode, bandwidth and latency are NOT working!\n",
    "emulator = emulation.Emulator(\n",
    "    emulation.CLUSTER_ABY3_3PC, mode, bandwidth=100, latency=10\n",
    ")\n",
    "# start up the running processes or containers\n",
    "emulator.up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here, we just use the mock dataset.\n",
    "# user can prepare your dataset: you need to always choose data that is close to reality!\n",
    "# Note: you should load data to PYU before run in SPU.\n",
    "# ref: https://www.secretflow.org.cn/docs/spu/en/getting_started/quick_start.html#Move-JAX-program-to-SPU\n",
    "(x1, x2), y = emulator.prepare_dataset(emulation.DATASET_MOCK_REGRESSION_BASIC)\n",
    "\n",
    "# start running program and get results\n",
    "result = emulator.run(run_model)(x1, x2, y)\n",
    "\n",
    "# For safety, can put the program in a try-catch block\n",
    "emulator.down()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Put them together\n",
    "\n",
    "Now we put all these together, we can get a simple paradigm of emulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# 0). Import library & define running function used by emulator\n",
    "###########################################################################\n",
    "import sml.utils.emulation as emulation\n",
    "\n",
    "\n",
    "# normally, we will import a Model that we have tested in simulator, e.g:\n",
    "# from model import Model\n",
    "\n",
    "def run_model(model, x1, x2, y):\n",
    "    x = jnp.concatenate((x1, x2), axis=1)\n",
    "    y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "    model.fit(x, y)\n",
    "    return model.predict_proba(x)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# 2). Define model and design experiments\n",
    "###########################################################################\n",
    "# you can use a loop to define your multiple experiment configs\n",
    "# for batch_size in [1024, 2048, 4096]\n",
    "# for sig_type in ['t1', 'sr']\n",
    "# ...\n",
    "model = SSLRSGDClassifier(epochs=5, learning_rate=0.1, batch_size=2048,\n",
    "                          sig_type='t1', eps=1e-4, dk_method='rsqrt')\n",
    "\n",
    "###########################################################################\n",
    "# 3). Define running config and run emulations\n",
    "###########################################################################\n",
    "#  Set mode to MULTIPROCESS for LAN test.\n",
    "mode = emulation.Mode.MULTIPROCESS  # emulation.Mode.DOCKER for docker not supported yet\n",
    "\n",
    "# bandwidth and latency only work for docker mode\n",
    "emulator = emulation.Emulator(\n",
    "    emulation.CLUSTER_ABY3_3PC, mode, bandwidth=100, latency=10\n",
    ")\n",
    "\n",
    "# For safety, it's a good practice that putting the running part in a try-catch block\n",
    "try:\n",
    "    # start up the running processes\n",
    "    emulator.up()\n",
    "\n",
    "    # prepare your dataset here.\n",
    "    #   a. Normally, you should choose your dataset carefully. e.g. for lr,\n",
    "    # we need to examine the performance on imbalanced, tail-heavy(or real dataset if possible) datasets.\n",
    "    #   b. If you just want to get some efficiency number, we have a mock api to produce dataset.\n",
    "    (x1, x2), y = emulator.prepare_dataset(emulation.DATASET_MOCK_REGRESSION_BASIC)\n",
    "\n",
    "    # magic happens here! running the program in emulator like SPU\n",
    "    result = emulator.run(run_model)(model, x1, x2, y)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    emulator.down()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Mar  2 2023, 03:21:46) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2cbe093a14a4c28eeca1f1e06597ee6d950d8ff7968d59e0b42df2ea4ce5b8a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
